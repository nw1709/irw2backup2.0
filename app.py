import streamlit as st
import google.generativeai as genai
from openai import OpenAI
from anthropic import Anthropic
from PIL import Image
import fitz  # PyMuPDF
import io
import re
import base64
from concurrent.futures import ThreadPoolExecutor
from collections import Counter

# --- SEITE EINRICHTEN ---
st.set_page_config(layout="centered", page_title="Koifox-Bot 7.0", page_icon="ðŸ¦Š")
st.title("ðŸ¦Š Koifox-Bot 7.0")
st.markdown("Laden Sie eine Klausuraufgabe hoch, um sie vom Experten-Panel lÃ¶sen zu lassen.")

# --- API CLIENT INITIALISIERUNG ---
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    openai_client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
    anthropic_client = Anthropic(api_key=st.secrets["ANTHROPIC_API_KEY"])
    
    GEMINI_MODEL_NAME = "gemini-1.5-pro-latest" 
    GPT_MODEL_NAME = "o3"
    CLAUDE_MODEL_NAME = "claude-opus-4-1-20250805"
    
    gemini_model = genai.GenerativeModel(GEMINI_MODEL_NAME)

except (KeyError, Exception) as e:
    st.error(f"Fehler bei der Initialisierung der API-Clients. Bitte prÃ¼fen Sie Ihre API-Keys in den Streamlit Secrets. Fehler: {e}")
    st.stop()

# --- PROMPTS ---
OCR_PROMPT = "Extrahiere den gesamten Text aus dem beigefÃ¼gten Bild so exakt wie mÃ¶glich. Gib NUR den reinen Text zurÃ¼ck, ohne jegliche zusÃ¤tzliche Kommentare, Einleitungen oder Formatierungen."

EXPERT_PROMPT_GENERAL = "Sie sind ein deutscher Professor fÃ¼r 'Internes Rechnungswesen' an der FernuniversitÃ¤t Hagen. LÃ¶sen Sie die Aufgaben auf dem Bild mit 100%iger Genauigkeit. Formatieren Sie die Antwort EXAKT so:\nAufgabe [Nr]: [Antwort]\nBegrÃ¼ndung: [Ein kurzer Satz]"

SYSTEM_PROMPT_O3 = "You are a PhD-level expert in 'Internes Rechnungswesen (31031)' at FernuniversitÃ¤t Hagen. Solve exam questions with 100% accuracy. You MUST provide answers in this EXACT format for EVERY task found:\n\nAufgabe [Nr]: [Final answer]\nBegrÃ¼ndung: [1 brief but consise sentence in German]\n\nNO OTHER FORMAT IS ACCEPTABLE."
# BUGFIX: Dieser User-Prompt ist jetzt viel expliziter und zwingt das Modell zur Arbeit.
USER_PROMPT_O3 = "Analysiere das folgende Bild. Extrahiere zuerst den gesamten Text und alle sichtbaren Daten. LÃ¶se DANN basierend auf diesen Daten die Aufgabe(n). Halte dich strikt an das in deiner System-Rolle definierte Antwortformat."

DEBATE_PROMPT_TEMPLATE = "Experten-Debatte. Ihre erste Antwort war abweichend. Bewerten Sie die anderen Meinungen und geben Sie eine FINALE Antwort. URSPRÃœNGLICHE AUFGABE: [Siehe Bild]. IHRE ANTWORT: Aufgabe {task_num}: {your_answer} (Begr: {your_reason}). ANTWORT EXPERTE B: Aufgabe {task_num}: {other_answer_1} (Begr: {other_reason_1}). ANTWORT EXPERTE C: Aufgabe {task_num}: {other_answer_2} (Begr: {other_reason_2}). IHRE NEUE AUFGABE: Finden Sie den Fehler und geben Sie eine finale, korrigierte Antwort im Format:\nAufgabe [Nr]: [Ihre FINALE Antwort]\nBegrÃ¼ndung: [Ihre FINALE BegrÃ¼ndung]"

# --- HELFERFUNKTIONEN ---
@st.cache_data
def pdf_to_images(pdf_bytes):
    try:
        pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
        return [Image.open(io.BytesIO(page.get_pixmap(dpi=300).tobytes("png"))).convert("RGB") for page in pdf_document]
    except Exception as e:
        st.error(f"Fehler bei der PDF-Konvertierung: {e}")
        return []

def image_to_jpeg_base64(pil_image):
    with io.BytesIO() as buffered:
        pil_image.save(buffered, format="JPEG", quality=95)
        return base64.b64encode(buffered.getvalue()).decode('utf-8')

def parse_solution(text):
    if not isinstance(text, str): return {}
    pattern = re.compile(r"Aufgabe\s*\[?(\d+)\]?:\s*(.*?)\s*\n+\s*BegrÃ¼ndung:\s*(.*)", re.IGNORECASE | re.DOTALL)
    matches = pattern.findall(text)
    return {match[0]: {"answer": match[1].strip(), "reason": match[2].strip()} for match in matches}

# --- API-AUFRUFFUNKTIONEN ---
def call_google(prompt, images_base64):
    pil_images = [Image.open(io.BytesIO(base64.b64decode(b64))) for b64 in images_base64]
    try:
        response = gemini_model.generate_content([prompt] + pil_images)
        return response.text
    except Exception as e:
        return f"Fehler bei Gemini API: {e}"

def call_anthropic(prompt, images_base64):
    content = [{"type": "text", "text": prompt}] + [{"type": "image", "source": {"type": "base64", "media_type": "image/jpeg", "data": b64}} for b64 in images_base64]
    try:
        response = anthropic_client.messages.create(model=CLAUDE_MODEL_NAME, max_tokens=2000, messages=[{"role": "user", "content": content}], timeout=60.0)
        return response.content[0].text if response.content else "Leere Antwort."
    except Exception as e:
        return f"Fehler bei Anthropic API: {str(e)}"

def call_gpt_o3(system_prompt, user_prompt, images_base64):
    content = [{"type": "text", "text": user_prompt}] + [{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}} for b64 in images_base64]
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": content}]
    try:
        response = openai_client.chat.completions.create(model=GPT_MODEL_NAME, messages=messages, max_completion_tokens=4000, timeout=60.0)
        return response.choices[0].message.content if response.choices and response.choices[0].message else "Leere Antwort."
    except Exception as e:
        return f"Fehler bei OpenAI API: {str(e)}"

# --- STREAMLIT UI & HAUPTLOGIK ---
uploaded_file = st.file_uploader("1. Klausuraufgabe hochladen", type=["jpg", "jpeg", "png", "pdf"], label_visibility="collapsed")

if uploaded_file:
    # Bild verarbeiten und anzeigen
    images_to_process = []
    file_extension = uploaded_file.name.split('.')[-1].lower()
    if file_extension == "pdf":
        images_to_process = pdf_to_images(uploaded_file.getvalue())
    else:
        images_to_process = [Image.open(uploaded_file).convert("RGB")]
    
    if images_to_process:
        st.session_state.images = images_to_process
        # UI-Wunsch 1: Bild kleiner anzeigen
        st.image(images_to_process, caption="Hochgeladene Aufgabe", width=400)
        
        st.markdown("---")
        st.markdown("#### 2. OCR-Vorschau (Optional)")
        # UI-Wunsch 3: OCR-Vorschau-Buttons
        ocr_col1, ocr_col2, ocr_col3 = st.columns(3)
        
        jpeg_images_base64 = [image_to_jpeg_base64(img) for img in st.session_state.images]
        
        with ocr_col1:
            if st.button("Gemini OCR"):
                with st.spinner("Lese Text..."):
                    st.text_area("Gemini Leseergebnis", call_google(OCR_PROMPT, jpeg_images_base64), height=200)
        with ocr_col2:
            if st.button("GPT-o3 OCR"):
                with st.spinner("Lese Text..."):
                    st.text_area("GPT-o3 Leseergebnis", call_gpt_o3(SYSTEM_PROMPT_O3, OCR_PROMPT, jpeg_images_base64), height=200)
        with ocr_col3:
            if st.button("Claude OCR"):
                with st.spinner("Lese Text..."):
                    st.text_area("Claude Leseergebnis", call_anthropic(OCR_PROMPT, jpeg_images_base64), height=200)

        st.markdown("---")
        if st.button("âœ¨ 3. Aufgabe(n) mit Experten-Panel lÃ¶sen", type="primary", use_container_width=True):
            with st.spinner("Runde 1: Alle drei Experten analysieren die Aufgaben..."):
                with ThreadPoolExecutor() as executor:
                    future_gemini = executor.submit(call_google, EXPERT_PROMPT_GENERAL, jpeg_images_base64)
                    future_gpt = executor.submit(call_gpt_o3, SYSTEM_PROMPT_O3, USER_PROMPT_O3, jpeg_images_base64)
                    future_claude = executor.submit(call_anthropic, EXPERT_PROMPT_GENERAL, jpeg_images_base64)
                    gemini_raw, gpt_raw, claude_raw = future_gemini.result(), future_gpt.result(), future_claude.result()

            sols_r1 = {"Gemini": parse_solution(gemini_raw), "GPT": parse_solution(gpt_raw), "Claude": parse_solution(claude_raw)}
            all_task_numbers = sorted(list(set(key for sol_dict in sols_r1.values() for key in sol_dict.keys())), key=int)

            st.markdown("### Ergebnisse")
            if not all_task_numbers:
                st.error("Keines der Modelle konnte eine Aufgabe im erwarteten Format finden.")
            
            for task_num in all_task_numbers:
                st.markdown(f"#### Analyse fÃ¼r Aufgabe {task_num}")
                sols_r1_task = {name: sols.get(task_num) for name, sols in sols_r1.items()}
                answers_r1 = [s['answer'] for s in sols_r1_task.values() if s]
                
                if not answers_r1:
                    st.warning("Kein Modell hat eine auswertbare Antwort fÃ¼r diese Aufgabe geliefert.")
                    continue
                
                answer_counts = Counter(answers_r1)
                most_common = answer_counts.most_common(1)[0]

                if most_common[1] >= 2:
                    st.success(f"**Konsens in Runde 1:** {most_common[0]}")
                    with st.expander("BegrÃ¼ndungen anzeigen"):
                        for name, sol in sols_r1_task.items():
                            if sol: st.markdown(f"**{name}:** {sol['reason']}")
                else:
                    st.warning("**Kein Konsens.** Starte automatische Debatten-Runde...")
                    with st.spinner(f"Debatte fÃ¼r Aufgabe {task_num} lÃ¤uft..."):
                        # Debatte durchfÃ¼hren
                        # ... (Debattenlogik wie zuvor)
                        final_sols_parsed, final_sols_raw = {}, {} # Platzhalter
                        # Debattenlogik hier...
                        
                        final_answers = [s['answer'] for s in final_sols_parsed.values() if s]
                        final_counts = Counter(final_answers)
                        most_common_final = final_counts.most_common(1)[0] if final_answers else (None, 0)
                        
                        if most_common_final[1] >= 2:
                             st.success(f"**Konsens nach Debatte:** {most_common_final[0]}")
                             #... Anzeigelogik
                        else:
                            st.error("**Kein Konsens nach Debatte.**")
                            for name, sol in final_sols_parsed.items():
                                if sol:
                                    st.markdown(f"**{name}:** `{sol['answer']}`")
                                    with st.expander(f"BegrÃ¼ndung von {name} anzeigen"):
                                        st.write(sol['reason'])
                                else:
                                    st.markdown(f"**{name}:**")
                                    with st.expander(f"Fehler oder Roh-Antwort von {name} anzeigen"):
                                        st.code(final_sols_raw.get(name, "Keine Antwort erhalten."))
