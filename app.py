import streamlit as st
from anthropic import Anthropic
from openai import OpenAI
from PIL import Image
import google.generativeai as genai
import logging
import hashlib
import re
from sentence_transformers import SentenceTransformer, util

# --- Logger Setup ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- API Key Validation ---
def validate_keys():
    required_keys = {
        'gemini_key': ('AIza', "Gemini"),
        'claude_key': ('sk-ant', "Claude"),
        'openai_key': ('sk-', "OpenAI")
    }
    missing = []
    invalid = []
    
    for key, (prefix, name) in required_keys.items():
        if key not in st.secrets:
            missing.append(name)
        elif not st.secrets[key].startswith(prefix):
            invalid.append(name)
    
    if missing or invalid:
        st.error(f"API Key Problem: Missing {', '.join(missing)} | Invalid {', '.join(invalid)}")
        st.stop()

validate_keys()

# --- UI-Einstellungen ---
st.set_page_config(layout="centered", page_title="3.0", page_icon="üåï")
st.title("ü¶ä Koifox-Bot")
st.markdown("*Verbesserte OCR f√ºr Graphen, strikte Formatierung & Konsistenzpr√ºfung*")

# --- Gemini Flash Konfiguration ---
genai.configure(api_key=st.secrets["gemini_key"])
vision_model = genai.GenerativeModel("gemini-1.5-flash")

# --- SentenceTransformer f√ºr Konsistenzpr√ºfung ---
@st.cache_resource
def load_sentence_transformer():
    return SentenceTransformer('all-MiniLM-L6-v2')

sentence_model = load_sentence_transformer()

# --- Verbesserte OCR mit Graphenbeschreibung ---
@st.cache_data(ttl=3600)
def extract_text_with_gemini_improved(_image, file_hash):
    """Extrahiert KOMPLETTEN Text und beschreibt Graphen"""
    try:
        logger.info(f"Starting GEMINI OCR for file hash: {file_hash}")
        
        # Bildskalierung beibehalten, da manuelle Vorverarbeitung erfolgt
        max_size = 3000
        if _image.width > max_size or _image.height > max_size:
            _image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
            st.sidebar.warning(f"Bild wurde auf {max_size}px skaliert")
        
        # Strukturierter OCR Prompt mit Graphenbeschreibung
        response = vision_model.generate_content(
            [
                """Extract ALL content from this exam image in a structured format, including text, formulas, and visual elements like graphs, diagrams, or tables.

                IMPORTANT:
                - Read ALL text from top to bottom, including:
                  - Question numbers (e.g., "Aufgabe 45 (5 RP)")
                  - All questions, formulas, values, and answer options (A, B, C, D, E) with their complete text
                  - Mathematical symbols and formulas exactly as shown (e.g., f(r‚ÇÅ,r‚ÇÇ) = (r‚ÇÅ^Œ± + r‚ÇÇ^Œ≤)^Œ≥)
                - For graphs, diagrams, or tables:
                  - Describe the visual elements in detail, including:
                    - Axes labels (e.g., x-axis: "Quantity", y-axis: "Cost")
                    - Data points or values shown (e.g., "Point at (10, 500)")
                    - Curve shapes or trends (e.g., "Linear increasing curve")
                    - Any annotations or labels in the graph
                  - If a table is present, extract it as a structured table (e.g., rows and columns)
                - Structure the output as follows:
                  - Aufgabe [Nummer]: [Fragetext]
                    - Option A: [Text]
                    - Option B: [Text]
                    - ...
                    - Formeln: [z. B. f(r‚ÇÅ,r‚ÇÇ) = (r‚ÇÅ^Œ± + r‚ÇÇ^Œ≤)^Œ≥]
                    - Graph/Table: [Detailed description]
                - DO NOT summarize or skip any part
                - DO NOT solve anything
                
                Start from the very top and continue to the very bottom of the image.""",
                _image
            ],
            generation_config={
                "temperature": 0,
                "max_output_tokens": 10000
            }
        )
        
        ocr_result = response.text.strip()
        
        # Pr√ºfe ob genug Text extrahiert wurde
        if len(ocr_result) < 500:
            st.warning(f"‚ö†Ô∏è Nur {len(ocr_result)} Zeichen extrahiert - m√∂glicherweise unvollst√§ndig! √úberpr√ºfe, ob Graphen/Diagramme erkannt wurden.")
        
        # Pr√ºfe auf Graphenbeschreibungen
        if "Graph:" not in ocr_result and "Table:" not in ocr_result:
            st.warning("‚ö†Ô∏è Keine Graphen oder Tabellen im OCR-Text gefunden. M√∂glicherweise wurden visuelle Elemente nicht erkannt.")
        
        logger.info(f"GEMINI OCR completed: {len(ocr_result)} characters")
        return ocr_result
        
    except Exception as e:
        logger.error(f"Gemini OCR Error: {str(e)}")
        raise e

# --- Konsistenzpr√ºfung zwischen LLMs ---
def are_answers_similar(answer1, answer2):
    """Vergleicht die Endantworten auf semantische √Ñhnlichkeit"""
    try:
        # Extrahiere nur die Endantworten (z.B. "11,25" oder "11.5")
        task_pattern = r'Aufgabe\s+\d+\s*:\s*([^\n]+)'
        answers1 = re.findall(task_pattern, answer1, re.IGNORECASE)
        answers2 = re.findall(task_pattern, answer2, re.IGNORECASE)
        
        if not answers1 or not answers2:
            logger.warning("Keine Endantworten f√ºr Konsistenzpr√ºfung gefunden")
            return False
        
        # Vergleiche nur die Endantworten
        embeddings = sentence_model.encode([' '.join(answers1), ' '.join(answers2)])
        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()
        logger.info(f"Antwort√§hnlichkeit (Endantworten): {similarity:.2f}")
        return similarity > 0.8, answers1, answers2
    except Exception as e:
        logger.error(f"Konsistenzpr√ºfung fehlgeschlagen: {str(e)}")
        return False, [], []

# --- Claude Solver mit strikter Formatierung ---
def solve_with_claude_formatted(ocr_text):
    """Claude l√∂st und formatiert korrekt mit Chain-of-Thought"""
    
    prompt = f"""Du bist ein Experte f√ºr "Internes Rechnungswesen (31031)" an der Fernuni Hagen.

VOLLST√ÑNDIGER AUFGABENTEXT:
{ocr_text}

WICHTIGE REGELN:
1. Identifiziere ALLE Aufgaben im Text (z.B. "Aufgabe 45", "Aufgabe 46" etc.)
2. Bei Homogenit√§t: f(r‚ÇÅ,r‚ÇÇ) = (r‚ÇÅ^Œ± + r‚ÇÇ^Œ≤)^Œ≥ ist NUR homogen wenn Œ± = Œ≤
3. Beantworte JEDE Aufgabe die du findest
4. Denke schrittweise:
   - Lies die Aufgabe sorgf√§ltig
   - Identifiziere alle relevanten Formeln, Werte und visuelle Daten (z.B. Graphenbeschreibungen)
   - Wenn Daten unvollst√§ndig sind, dokumentiere Annahmen klar
   - F√ºhre die Berechnung explizit durch
   - √úberpr√ºfe dein Ergebnis
5. Bei Multiple-Choice-Fragen: Analysiere jede Option und begr√ºnde, warum sie richtig oder falsch ist
6. Wenn Graphen oder Tabellen beschrieben sind, nutze diese Informationen f√ºr die L√∂sung
7. Die Endantwort MUSS exakt der berechneten Zahl entsprechen (z.B. 11.5, nicht 11,25) und auf zwei Dezimalstellen formatiert sein

AUSGABEFORMAT (STRIKT EINHALTEN):
Aufgabe [Nummer]: [Antwort auf zwei Dezimalstellen]
Begr√ºndung: [Schritt-f√ºr-Schritt-Erkl√§rung]
Berechnung: [Mathematische Schritte]
Annahmen (falls n√∂tig): [z.B. "Fehlende Datenpunkte im Graphen wurden als linear angenommen"]

Wiederhole dies f√ºr JEDE Aufgabe im Text.

Beispiel:
Aufgabe 45: 500.00
Begr√ºndung: Der Parameter a entspricht dem Achsenabschnitt bei p = 0, also a = 500.
Berechnung: a = f(0) = 500
Annahmen: Keine

Aufgabe 46: 11.50
Begr√ºndung: Um Parameter b zu bestimmen...
Berechnung: b = (f(1) - f(0)) / x
Annahmen: Linearer Kurvenverlauf basierend auf Graphenbeschreibung

WICHTIG: Vergiss keine Aufgabe!"""

    client = Anthropic(api_key=st.secrets["claude_key"])
    response = client.messages.create(
        model="claude-4-opus-20250514",
        max_tokens=4000,
        temperature=0.1,
        system="Beantworte ALLE Aufgaben die im Text stehen. √úberspringe keine. Stelle sicher, dass die Endantwort exakt der Berechnung entspricht.",
        messages=[{"role": "user", "content": prompt}]
    )
    
    return response.content[0].text

# --- GPT-4 Turbo Solver ---
def solve_with_gpt(ocr_text):
    """GPT-4 Turbo l√∂st mit Chain-of-Thought"""
    
    prompt = f"""Du bist ein Experte f√ºr "Internes Rechnungswesen (31031)" an der Fernuni Hagen.

VOLLST√ÑNDIGER AUFGABENTEXT:
{ocr_text}

WICHTIGE REGELN:
1. Identifiziere ALLE Aufgaben im Text (z.B. "Aufgabe 45", "Aufgabe 46" etc.)
2. Bei Homogenit√§t: f(r‚ÇÅ,r‚ÇÇ) = (r‚ÇÅ^Œ± + r‚ÇÇ^Œ≤)^Œ≥ ist NUR homogen wenn Œ± = Œ≤
3. Beantworte JEDE Aufgabe die du findest
4. Denke schrittweise:
   - Lies die Aufgabe sorgf√§ltig
   - Identifiziere alle relevanten Formeln, Werte und visuelle Daten (z.B. Graphenbeschreibungen)
   - Wenn Daten unvollst√§ndig sind, dokumentiere Annahmen klar
   - F√ºhre die Berechnung explizit durch
   - √úberpr√ºfe dein Ergebnis
5. Bei Multiple-Choice-Fragen: Analysiere jede Option und begrunde, warum sie richtig oder falsch ist
6. Wenn Graphen oder Tabellen beschrieben sind, nutze diese Informationen f√ºr die L√∂sung
7. Die Endantwort MUSS exakt der berechneten Zahl entsprechen (z.B. 11.5, nicht 11,25) und auf zwei Dezimalstellen formatiert sein

AUSGABEFORMAT (STRIKT EINHALTEN):
Aufgabe [Nummer]: [Antwort auf zwei Dezimalstellen]
Begr√ºndung: [Schritt-f√ºr-Schritt-Erkl√§rung]
Berechnung: [Mathematische Schritte]
Annahmen (falls n√∂tig): [z.B. "Fehlende Datenpunkte im Graphen wurden als linear angenommen"]

Wiederhole dies f√ºr JEDE Aufgabe im Text."""

    client = OpenAI(api_key=st.secrets["openai_key"])
    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": "Beantworte ALLE Aufgaben die im Text stehen. √úberspringe keine. Stelle sicher, dass die Endantwort exakt der Berechnung entspricht."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=4000,
        temperature=0.1
    )
    
    return response.choices[0].message.content

# --- Verbesserte Ausgabeformatierung mit Konsistenzpr√ºfung ---
def parse_and_display_solution(solution_text, model_name="Claude"):
    """Parst und zeigt L√∂sung strukturiert an, pr√ºft Konsistenz mit Berechnung"""
    
    # Finde alle Aufgaben mit Regex
    task_pattern = r'Aufgabe\s+(\d+)\s*:\s*([^\n]+)'
    tasks = re.findall(task_pattern, solution_text, re.IGNORECASE)
    
    if not tasks:
        st.warning(f"‚ö†Ô∏è Keine Aufgaben im erwarteten Format gefunden ({model_name})")
        st.markdown(solution_text)
        return
    
    # Zeige jede Aufgabe strukturiert
    for task_num, answer in tasks:
        st.markdown(f"### Aufgabe {task_num}: **{answer.strip()}** ({model_name})")
        
        # Finde zugeh√∂rige Begr√ºndung, Berechnung und Annahmen
        begr_pattern = rf'Aufgabe\s+{task_num}\s*:.*?\n\s*Begr√ºndung:\s*([^\n]+(?:\n(?!Aufgabe)[^\n]+)*?)(?:\n\s*Berechnung:\s*([^\n]+(?:\n(?!Aufgabe)[^\n]+)*))?(?:\n\s*Annahmen\s*\(falls\s*n√∂tig\):\s*([^\n]+(?:\n(?!Aufgabe)[^\n]+)*))?(?=\n\s*Aufgabe|\Z)'
        begr_match = re.search(begr_pattern, solution_text, re.IGNORECASE | re.DOTALL)
        
        if begr_match:
            st.markdown(f"*Begr√ºndung: {begr_match.group(1).strip()}*")
            if begr_match.group(2):
                st.markdown(f"*Berechnung: {begr_match.group(2).strip()}*")
                # Pr√ºfe Konsistenz zwischen Endantwort und Berechnung
                calc_pattern = r'p\s*=\s*([\d,.]+)'
                calc_match = re.search(calc_pattern, begr_match.group(2), re.IGNORECASE)
                if calc_match:
                    calc_answer = calc_match.group(1).replace(',', '.')
                    if calc_answer != answer.strip():
                        st.warning(f"‚ö†Ô∏è Inkonsistenz in Aufgabe {task_num} ({model_name}): Endantwort ({answer.strip()}) unterscheidet sich von Berechnung ({calc_answer})")
            if begr_match.group(3):
                st.markdown(f"*Annahmen: {begr_match.group(3).strip()}*")
        
        st.markdown("---")

# --- UI ---
# Cache leeren
if st.sidebar.button("üóëÔ∏è Clear Cache"):
    st.cache_data.clear()
    st.rerun()

# Debug
debug_mode = st.checkbox("üîç Debug-Modus", value=True)

# Datei-Upload
uploaded_file = st.file_uploader(
    "**Klausuraufgabe hochladen...**",
    type=["png", "jpg", "jpeg"]
)

if uploaded_file is not None:
    try:
        # Bild verarbeiten
        file_hash = hashlib.md5(uploaded_file.getvalue()).hexdigest()
        image = Image.open(uploaded_file)
        
        # Zeige Original
        st.image(image, caption=f"Originalbild ({image.width}x{image.height}px)", use_container_width=True)
        
        # OCR
        with st.spinner("üìñ Lese KOMPLETTEN Text und Graphen mit Gemini..."):
            ocr_text = extract_text_with_gemini_improved(image, file_hash)
        
        # OCR Ergebnis
        with st.expander(f"üîç OCR-Ergebnis ({len(ocr_text)} Zeichen)", expanded=debug_mode):
            st.code(ocr_text)
            
            # Pr√ºfe ob Aufgaben gefunden wurden
            found_tasks = re.findall(r'Aufgabe\s+\d+', ocr_text, re.IGNORECASE)
            if found_tasks:
                st.success(f"‚úÖ Gefundene Aufgaben: {', '.join(found_tasks)}")
            else:
                st.error("‚ùå Keine Aufgaben im Text gefunden!")
            
            # Pr√ºfe auf Graphenbeschreibungen
            if "Graph:" in ocr_text or "Table:" in ocr_text:
                st.success("‚úÖ Graphen oder Tabellen im OCR-Text gefunden!")
        
        # L√∂sen
        if st.button("üßÆ Alle Aufgaben l√∂sen", type="primary"):
            st.markdown("---")
            
            with st.spinner("üßÆ Claude und GPT-4 l√∂sen ALLE Aufgaben..."):
                claude_solution = solve_with_claude_formatted(ocr_text)
                gpt_solution = solve_with_gpt(ocr_text)
                
                # Konsistenzpr√ºfung
                is_similar, claude_answers, gpt_answers = are_answers_similar(claude_solution, gpt_solution)
                if is_similar:
                    st.success("‚úÖ Beide Modelle sind einig!")
                    st.markdown("### üìä L√∂sungen (Claude):")
                    parse_and_display_solution(claude_solution, model_name="Claude")
                else:
                    st.warning("‚ö†Ô∏è Modelle uneinig! Zeige beide L√∂sungen zur √úberpr√ºfung.")
                    st.markdown("### üìä L√∂sungen (Claude):")
                    parse_and_display_solution(claude_solution, model_name="Claude")
                    st.markdown("### üìä L√∂sungen (GPT-4 Turbo):")
                    parse_and_display_solution(gpt_solution, model_name="GPT-4 Turbo")
                    # Zeige Unterschiede in Endantworten
                    st.markdown("### Unterschiede in Endantworten:")
                    for i, (c_answer, g_answer) in enumerate(zip(claude_answers, gpt_answers)):
                        if c_answer != g_answer:
                            st.markdown(f"- Aufgabe {found_tasks[i].split()[-1]}: Claude: **{c_answer}**, GPT-4: **{g_answer}**")
            
            if debug_mode:
                with st.expander("üí≠ Rohe Claude-Antwort"):
                    st.code(claude_solution)
                with st.expander("üí≠ Rohe GPT-4-Antwort"):
                    st.code(gpt_solution)
                    
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        st.error(f"‚ùå Fehler: {str(e)}")

# --- Footer ---
st.markdown("---")
st.caption("Koifox-Bot | Strikt formatierte L√∂sungen, Graphen-OCR & Konsistenzpr√ºfung")
